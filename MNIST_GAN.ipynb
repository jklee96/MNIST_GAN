{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jklee96/MNIST_GAN/blob/main/MNIST_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 실습 준비: 라이브러리 불러오기**\n",
        "\n",
        "MNIST 데이터를 사용하여 생성적 적대 신경망(Generative Adversarial Network, GAN)을 구현하고 학습시키는 예제입니다.\n",
        "\n",
        "먼저, 모델 구현에 필요한 PyTorch 관련 라이브러리와 데이터 시각화를 위한 matplotlib, numpy 등을 불러옵니다."
      ],
      "metadata": {
        "id": "aJMIn7A4ipoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "Dn9ZZEV5l80p"
      },
      "outputs": [],
      "source": [
        "# Pytorch 관련 기본라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from PIL import Image\n",
        "\n",
        "# torchvision 라이브러리 (데이터셋 및 변환 기능 제공)\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 시각화 및 계산을 위한 라이브러리\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 학습 환경 및 하이퍼파라미터 설정**\n",
        "\n",
        "본격적인 모델 구현에 앞서, 학습을 위한 기본 환경과 하이퍼파라미터(Hyperparameter)를 설정합니다.\n",
        "\n",
        "먼저, torch.cuda.is_available() 코드를 통해 GPU 사용 가능 여부를 확인하고, 사용 가능한 경우 연산 장치를 cuda(GPU)로 설정합니다.\n",
        "\n",
        "이후 GAN 모델 학습에 필요한 주요 하이퍼파라미터를 정의합니다. 하이퍼파라미터는 모델이 스스로 학습하는 값이 아니라, 개발자가 더 나은 학습 결과를 위해 직접 설정하는 값들을 의미합니다.\n",
        "\n",
        "**<주요 구성 요소>**\n",
        "\n",
        "latent_dim: 생성자(Generator)가 이미지를 만들 때 사용할 무작위 노이즈 벡터의 크기입니다. 이 노이즈가 이미지의 '재료'가 됩니다.\n",
        "\n",
        "epochs: 전체 데이터셋을 몇 번 반복하여 학습할지를 결정합니다.\n",
        "\n",
        "learning_rate: 학습률은 모델이 정답을 찾아가는 과정에서 가중치(weight)를 얼마나 크게 조정할지를 나타내는 값입니다.\n",
        "\n",
        "output_dir: 학습 과정에서 생성된 이미지를 저장할 폴더의 이름입니다.\n",
        "\n",
        "batch_size: 한 번에 학습할 데이터의 묶음(batch) 크기를 의미합니다."
      ],
      "metadata": {
        "id": "IvCt-gjCt_2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 가능 여부 확인 후 device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# GAN을 위한 하이퍼파라미터\n",
        "latent_dim = 100 # Generator의 랜덤 노이즈 입력 차원\n",
        "epochs = 10 # 학습 에포크 수\n",
        "learning_rate = 0.0002 # Generator와 Discriminator 모두에 대한 학습률\n",
        "output_dir = './GAN_output' # 생성된 이미지를 저장할 폴더\n",
        "batch_size = 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZhcYxmEXKea",
        "outputId": "7e2ff001-9f32-494b-9187-bbcaa983cba6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 데이터 준비: MNIST 데이터셋 불러오기**\n",
        "\n",
        "모델을 학습시키려면 '데이터'가 필요합니다. 이번 실습에서는 손으로 쓴 숫자 이미지로 구성된 MNIST 데이터셋을 사용합니다.\n",
        "\n",
        "컴퓨터가 이미지를 바로 이해할 수는 없기 때문에, 모델이 학습하기 좋은 형태로 데이터를 가공(전처리)하는 과정이 필요합니다.\n",
        "\n",
        "\n",
        "transforms.ToTensor(): 이미지를 PyTorch가 다룰 수 있는 텐서(Tensor) 자료형으로 변환합니다.\n",
        "\n",
        "transforms.Normalize(): 이미지 픽셀 값을 -1에서 1 사이의 값으로 정규화합니다. 이는 모델의 출력 범위와 맞춰주고, 학습을 더 안정적으로 만듭니다.\n",
        "\n",
        "\n",
        "그 다음, torchvision을 이용해 MNIST 훈련(train) 데이터와 테스트(test) 데이터를 모두 다운로드하고, ConcatDataset으로 두 데이터를 하나로 합쳐 더 많은 양의 데이터를 학습에 사용합니다.\n",
        "\n",
        "마지막으로, 준비된 전체 데이터셋을 DataLoader에 전달합니다. DataLoader는 데이터를 우리가 설정한 batch_size(64)만큼 작은 묶음으로 알아서 나눠주고, 학습 때마다 데이터의 순서를 무작위로 섞어 모델이 데이터의 순서까지 외우는 것을 방지하는 등 데이터를 효율적으로 공급하는 역할을 합니다."
      ],
      "metadata": {
        "id": "oGbzV3_gukCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------데이터로더 준비------------------------------#\n",
        "# MNIST 훈련 데이터셋 및 테스트 데이터셋 다운로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Tanh를 사용하므로 픽셀 값을 [0, 1]에서 [-1, 1]로 정규화합니다.\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# 훈련 데이터셋과 테스트 데이터셋을 하나로 합치기\n",
        "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n",
        "\n",
        "# DataLoader 객체 생성 (배치 단위로 데이터를 로드)\n",
        "train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader는 더 이상 사용되지 않으므로 제거하거나 주석 처리합니다.\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "kYHyPae6WfkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff65b6a5-9012-4b2f-9665-1577d81c18f9",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 343kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.54MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 모델 구현 (1): 생성자 (Generator) 만들기**\n",
        "\n",
        "GAN은 생성자(Generator)와 판별자(Discriminator)라는 두 개의 모델이 서로 경쟁하며 학습하는 구조입니다.\n",
        "\n",
        "이번 셀에서는 생성자 모델을 만듭니다. 생성자의 역할은 '가짜' 이미지를 만드는 '위조지폐범'과 같습니다. 아무것도 없는 상태에서 진짜 같은 가짜 숫자 이미지를 만들어내는 것이 목표입니다.\n",
        "\n",
        "<생성 과정>\n",
        "\n",
        "생성자는 latent_dim(100) 차원의 무작위 노이즈(noise) 벡터를 입력으로 받습니다. 이 노이즈는 어떤 이미지를 만들지에 대한 '재료'가 됩니다.\n",
        "\n",
        "이 작은 노이즈 덩어리를 ConvTranspose2d 라는 layer를 여러 번 통과시키면서, 마치 점토를 펴서 넓히듯이 이미지의 크기를 점차 키워나갑니다.\n",
        "\n",
        "(100x1x1 노이즈) → (256x7x7) → (128x14x14) → (64x28x28) → (1x28x28 최종 이미지)\n",
        "\n",
        "최종적으로, Tanh 활성화 함수를 통해 이미지 픽셀 값을 -1에서 1 사이로 조정합니다. 이는 우리가 앞에서 실제 MNIST 데이터의 픽셀 값을 변환했던 범위와 똑같이 맞춰주기 위함입니다.\n",
        "\n",
        "**<주요 구성 요소>**\n",
        "\n",
        "ConvTranspose2d: 이미지의 크기(해상도)를 키우는 업샘플링(upsampling) 역할을 하는 핵심 층입니다.\n",
        "\n",
        "BatchNorm2d: 각 층에서 데이터의 분포를 정규화하여 학습을 안정화시키는 역할을 합니다.\n",
        "\n",
        "ReLU: 모델이 더 복잡하고 다양한 패턴을 학습할 수 있도록 돕는 대표적인 활성화 함수입니다.\n",
        "\n",
        "Tanh: 결과물의 픽셀 값을 -1에서 1 사이로 만들어주는 활성화 함수입니다."
      ],
      "metadata": {
        "id": "4-OgBLIxuunE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------모델 준비------------------------------#\n",
        "# Generator의 입력으로 사용할 랜덤 노이즈 생성 함수\n",
        "def generate_noise(batch_size, latent_dim, device):\n",
        "    return torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
        "\n",
        "# Generator 클래스 정의: ConvTranspose2d로 업샘플링하여 노이즈를 28x28 이미지로 변환\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # 입력: 잠재 벡터 (latent_dim x 1 x 1)\n",
        "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=7, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # 출력: 256 x 7 x 7\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # 출력: 128 x 14 x 14\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # 출력: 64 x 28 x 28\n",
        "\n",
        "            # 출력 채널을 1로 만들고, 픽셀 값을 [-1, 1] 범위로 조정\n",
        "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Tanh() # Tanh 활성화 함수 사용\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "fM-CHJsvWl1i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 모델 구현 (2): 판별자 (Discriminator) 만들기**\n",
        "\n",
        "생성자(Generator)가 '위조지폐범'이라면, 판별자(Discriminator)는 위조지폐를 감별하는 '경찰' 또는 '예술품 감정사'와 같습니다.\n",
        "\n",
        "판별자의 역할은 이미지를 입력받아, 그 이미지가 실제 데이터(real)인지 아니면 생성자가 만들어낸 가짜 데이터(fake)인지를 구별해내는 것입니다. 이 판별 결과를 생성자에게 알려주면, 생성자는 판별자를 더 잘 속이기 위해 이미지를 개선해 나갑니다.\n",
        "\n",
        "판별 과정\n",
        "판별자는 생성자와 정반대의 구조를 가집니다. 28x28 크기의 이미지를 입력받아, Conv2d 층을 통과시키며 이미지에서 특징(feature)을 추출하고 크기를 점차 줄여나갑니다.\n",
        "\n",
        "(1x28x28 이미지) → (64x14x14) → (128x7x7) → ... → (최종 판별 결과)\n",
        "\n",
        "여러 층을 거쳐 압축된 특징 정보를 바탕으로, 최종적으로 Sigmoid 활성화 함수를 통해 0과 1 사이의 숫자 하나를 출력합니다.\n",
        "\n",
        "이 숫자가 1에 가까우면 '진짜' 이미지라고, 0에 가까우면 '가짜' 이미지라고 판단하는 것입니다.\n",
        "\n",
        "**<주요 구성 요소>**\n",
        "\n",
        "Conv2d: 이미지의 특징을 추출하고 크기를 줄이는 다운샘플링(downsampling) 역할을 합니다.\n",
        "\n",
        "LeakyReLU: ReLU와 비슷하지만, GAN의 학습 안정성을 높여주는 것으로 알려진 활성화 함수입니다.\n",
        "\n",
        "Sigmoid: 결과값을 0과 1 사이의 확률로 만들어, 모델이 '진짜' 또는 '가짜'라는 최종 판별을 내릴 수 있게 합니다."
      ],
      "metadata": {
        "id": "hxZMQtKpw939"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator 클래스 정의: Conv2d로 다운샘플링하여 28x28 이미지를 실제/가짜로 분류\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # 입력: 1 x 28 x 28 이미지\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 출력: 64 x 14 x 14\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 출력: 128 x 7 x 7\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=7, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 출력: 256 x 1 x 1\n",
        "\n",
        "            # 최종적으로 실제/가짜를 판별하는 단일 값 출력\n",
        "            nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 출력 모양을 [batch_size, 1]로 맞추기 위해 view 사용\n",
        "        return self.net(x).view(-1, 1)"
      ],
      "metadata": {
        "id": "TCRlUpeYXZti"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. 모델 생성 및 GPU 할당**\n",
        "\n",
        "이제 앞서 설계한 Generator와 Discriminator 클래스를 이용해 실제 모델을 만듭니다.\n",
        "\n",
        "클래스가 '붕어빵 틀'이라면, 이 코드는 틀을 사용해 실제 '붕어빵'(모델 객체)을 찍어내는 과정과 같습니다.\n",
        "\n",
        "이렇게 생성된 generator와 discriminator 모델은 .to(device) 명령을 통해 GPU에서 계산을 수행하도록 할당됩니다. 복잡한 연산을 GPU가 전담하게\n",
        "\n",
        " 만들어 학습 속도를 크게 높여주는 과정입니다."
      ],
      "metadata": {
        "id": "K2H9I-MfxNM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator와 Discriminator 모델을 인스턴스화\n",
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)"
      ],
      "metadata": {
        "id": "yhUE-xbYXgdK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. 손실 함수와 옵티마이저 설정**\n",
        "\n",
        "\n",
        "모델을 학습시키려면, 모델이 얼마나 잘하고 있는지를 평가하는 '평가 기준(손실 함수)'과, 그 평가를 바탕으로 모델을 개선하는 '개선 방법(옵티마이저)'이 필요합니다.\n",
        "\n",
        "손실 함수 (Loss Function): '정답'과의 거리 측정하기\n",
        "\n",
        "손실 함수는 모델의 예측이 정답과 얼마나 다른지를 나타내는 '오차' 또는 '손실(loss)' 값을 계산합니다. 모델은 이 손실 값을 최소화하는 방향으로 학습을 진행합니다.\n",
        "\n",
        "nn.BCEWithLogitsLoss: GAN은 이미지를 '진짜(1)' 또는 '가짜(0)'로 판별하는 이진 분류 문제이므로, 이진 교차 엔트로피(Binary Cross Entropy, BCE) 손실 함수를 사용합니다. BCEWithLogitsLoss는 마지막 단계의 Sigmoid 함수를 내장하여 계산 과정의 수치적 안정성을 높여주는 역할을 합니다.\n",
        "\n",
        "옵티마이저 (Optimizer): 똑똑하게 정답 찾아가기\n",
        "\n",
        "옵티마이저는 손실 함수가 계산한 '오차' 정보를 바탕으로, 모델의 파라미터(가중치)를 더 나은 방향으로 업데이트하는 역할을 합니다. 손실이라는 언덕을 가장 빠르고 효율적으로 내려가는 방법을 찾는 등산가와 같습니다.\n",
        "\n",
        "torch.optim.Adam: Adam은 현재 가장 널리 사용되는 효율적인 옵티마이저 중 하나입니다.\n",
        "\n",
        "두 개의 옵티마이저: 생성자와 판별자는 서로 경쟁하며 각자의 목표를 향해 학습하는 별개의 모델입니다. 따라서 각각의 파라미터를 업데이트하기 위한 generator_optimizer와 discriminator_optimizer를 따로 설정해줍니다."
      ],
      "metadata": {
        "id": "MlfFBgtUxtJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 정의\n",
        "# criterion = nn.BCELoss()\n",
        "criterion = nn.BCEWithLogitsLoss() # 수정된 코드\n",
        "\n",
        "# Optimizer 설정\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate,betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "CkZLyOBjWruq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. 학습 결과 저장을 위한 준비**\n",
        "\n",
        "본격적인 학습에 앞서, 생성자가 만들어내는 이미지를 저장하기 위한 마지막 준비를 합니다.\n",
        "\n",
        "먼저, 이전 학습 결과가 남아있지 않도록 GAN_output 폴더를 깨끗하게 비웁니다. 그 다음, 학습 중에 생성된 이미지를 PNG 파일로 변환하여 저장해주는 save_generated_images 함수를 정의합니다.\n",
        "\n",
        "이 과정을 통해, 우리는 매 학습 단계마다 생성된 이미지들을 직접 눈으로 확인하며 모델의 성능이 향상되는 과정을 추적할 수 있습니다."
      ],
      "metadata": {
        "id": "-WhgBAMYynTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------학습 준비------------------------------#\n",
        "# 학습 전에 GAN_output 폴더 비우기 (기존에 생성된 이미지 삭제)\n",
        "if os.path.exists(output_dir):\n",
        "   shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir)\n",
        "\n",
        "# 각 에포크 후에 생성된 이미지를 저장하는 함수 (기존 함수 그대로 사용)\n",
        "def save_generated_images(images, epoch, output_dir='./GAN_output'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 이미지를 numpy 배열로 변환하고 [0, 255] 범위로 스케일 조정\n",
        "    # MNIST 이미지는 이미 [0, 1] 범위이므로 255를 곱하고 uint8로 변환\n",
        "    images = images.detach().cpu().numpy().squeeze(1)\n",
        "    images = (images * 255).astype(np.uint8)\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        img = Image.fromarray(images[i], mode='L')  # 그레이스케일 이미지\n",
        "        img.save(os.path.join(output_dir, f'generated_image_epoch{epoch}_{i}.png'))\n",
        "\n"
      ],
      "metadata": {
        "id": "g9CzFffKWyDC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. 모델 학습: 생성자와 판별자의 경쟁**\n",
        "\n",
        "생성자(위조지폐범)와 판별자(경찰)의 본격적인 경쟁을 통해 서로를 성장시키는 학습 과정을 시작하겠습니다\n",
        "\n",
        "이 전체 학습은 epochs 수만큼 반복되며, 각 단계는 다음과 같이 두 부분으로 나뉩니다.\n",
        "\n",
        "**1. 판별자의 훈련 시간**\n",
        "\n",
        "먼저 판별자를 훈련시킵니다. 판별자의 목표는 '진짜'는 '진짜'로, '가짜'는 '가짜'로 완벽하게 구별해내는 능력을 키우는 것입니다.\n",
        "\n",
        "진짜 이미지 학습: 실제 MNIST 이미지를 보여주고 \"이건 진짜(레이블=1)야\"라고 알려줍니다. 판별자는 이 이미지를 보고 1에 가까운 값을 예측하도록 학습합니다.\n",
        "\n",
        "가짜 이미지 학습: 생성자가 만든 가짜 이미지를 보여주고 \"이건 가짜(레이블=0)야\"라고 알려줍니다. 판별자는 이 이미지를 보고 0에 가까운 값을 예측하도록 학습합니다.\n",
        "\n",
        "능력치 업데이트: 위 두 과정에서 발생한 총 오차(loss)를 바탕으로, 옵티마이저가 판별자의 감식안을 더 날카롭게 업데이트합니다.\n",
        "\n",
        "**2. 생성자의 훈련 시간**\n",
        "\n",
        "판별자의 훈련이 끝나면, 이번에는 생성자를 훈련시킬 차례입니다. 생성자의 유일한 목표는 판별자를 완벽하게 속여서, 자신이 만든 '가짜' 이미지를 판별자가 '진짜'라고 착각하게 만드는 것입니다.\n",
        "\n",
        "판별자 속이기: 생성자는 자신이 만든 가짜 이미지를 다시 판별자에게 보여줍니다.\n",
        "\n",
        "'진짜'라고 우기기: 그리고 판별자가 그 이미지를 보고 '진짜(레이블=1)*라고 판단하도록 속입니다. 판별자가 진짜라고 착각할수록 생성자의 손실(loss)은 낮아집니다.\n",
        "\n",
        "위조 기술 업데이트: 판별자가 얼마나 속지 않았는지를 나타내는 오차를 바탕으로, 옵티마이저가 생성자의 이미지 생성 기술을 더 정교하게 업데이트합니다.\n",
        "\n",
        "이 두 과정이 수많은 에포크(epoch) 동안 빠르게 반복됩니다. 훈련이 끝난 후에는 매 에포크마다 저장된 이미지들을 확인하며, 생성자가 점차 진짜 같은 숫자 이미지를 만들어내는 놀라운 과정을 관찰할 수 있습니다!"
      ],
      "metadata": {
        "id": "uaK9k7hhy96z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------GAN 학습------------------------------#\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader): # DataLoader에서 이미지와 레이블을 가져옴\n",
        "        real_images = images.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # 실제 레이블(1)과 가짜 레이블(0) 생성 및 [batch_size, 1]로 reshape\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "        ### Discriminator 학습 ###\n",
        "        # 실제 이미지에 대한 순전파\n",
        "        outputs = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels) # 실제 이미지는 실제 레이블과 일치해야 함\n",
        "\n",
        "        # 가짜 이미지 생성\n",
        "        noise = generate_noise(batch_size, latent_dim, device=device)\n",
        "        fake_images = generator(noise)\n",
        "\n",
        "        # 가짜 이미지에 대한 순전파\n",
        "        outputs = discriminator(fake_images.detach()) # detach로 Generator의 그라디언트가 업데이트되지 않도록 함\n",
        "        d_loss_fake = criterion(outputs, fake_labels) # 가짜 이미지는 가짜 레이블과 일치해야 함\n",
        "\n",
        "        # Discriminator에 대한 역전파 및 최적화\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        d_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        ### Generator 학습 ###\n",
        "        # 가짜 이미지 생성 및 Discriminator의 예측 가져오기\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels) # Generator는 가짜 이미지를 실제로 분류되도록 학습해야 함\n",
        "\n",
        "        # Generator에 대한 역전파 및 최적화\n",
        "        generator_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # 일정 스텝마다 손실 출력\n",
        "        if (i+1) % 100 == 0:\n",
        "             print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')\n",
        "\n",
        "\n",
        "    # 각 에포크 후에 생성된 이미지를 저장\n",
        "    with torch.no_grad():\n",
        "        test_noise = generate_noise(10, latent_dim, device=device) # 테스트용 이미지 10개 생성\n",
        "        generated_images = generator(test_noise)\n",
        "        save_generated_images(generated_images, epoch)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}] completed. Generated images saved.')"
      ],
      "metadata": {
        "id": "G3QRNjf2W1g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6241271-5517-4b9d-a00f-98b1a4f03547"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/1094], D_loss: 1.1223, G_loss: 0.9216\n",
            "Epoch [1/10], Step [200/1094], D_loss: 1.1320, G_loss: 0.9336\n",
            "Epoch [1/10], Step [300/1094], D_loss: 1.0637, G_loss: 0.9307\n",
            "Epoch [1/10], Step [400/1094], D_loss: 1.1443, G_loss: 0.9685\n",
            "Epoch [1/10], Step [500/1094], D_loss: 1.1439, G_loss: 0.9401\n",
            "Epoch [1/10], Step [600/1094], D_loss: 1.2803, G_loss: 0.9657\n",
            "Epoch [1/10], Step [700/1094], D_loss: 1.2285, G_loss: 0.9180\n",
            "Epoch [1/10], Step [800/1094], D_loss: 1.1335, G_loss: 0.9406\n",
            "Epoch [1/10], Step [900/1094], D_loss: 1.1710, G_loss: 0.9368\n",
            "Epoch [1/10], Step [1000/1094], D_loss: 1.2073, G_loss: 0.9163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2143205352.py:17: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  img = Image.fromarray(images[i], mode='L')  # 그레이스케일 이미지\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] completed. Generated images saved.\n",
            "Epoch [2/10], Step [100/1094], D_loss: 1.1735, G_loss: 1.0331\n",
            "Epoch [2/10], Step [200/1094], D_loss: 1.0647, G_loss: 1.0667\n",
            "Epoch [2/10], Step [300/1094], D_loss: 1.1046, G_loss: 1.0425\n",
            "Epoch [2/10], Step [400/1094], D_loss: 1.1900, G_loss: 0.9655\n",
            "Epoch [2/10], Step [500/1094], D_loss: 1.1454, G_loss: 0.9718\n",
            "Epoch [2/10], Step [600/1094], D_loss: 1.1699, G_loss: 0.9473\n",
            "Epoch [2/10], Step [700/1094], D_loss: 1.1683, G_loss: 1.0196\n",
            "Epoch [2/10], Step [800/1094], D_loss: 1.0880, G_loss: 1.0377\n",
            "Epoch [2/10], Step [900/1094], D_loss: 1.1694, G_loss: 1.0436\n",
            "Epoch [2/10], Step [1000/1094], D_loss: 1.1215, G_loss: 0.9732\n",
            "Epoch [2/10] completed. Generated images saved.\n",
            "Epoch [3/10], Step [100/1094], D_loss: 1.1557, G_loss: 0.9673\n",
            "Epoch [3/10], Step [200/1094], D_loss: 1.0969, G_loss: 0.9934\n",
            "Epoch [3/10], Step [300/1094], D_loss: 1.1634, G_loss: 0.9363\n",
            "Epoch [3/10], Step [400/1094], D_loss: 1.2121, G_loss: 0.9324\n",
            "Epoch [3/10], Step [500/1094], D_loss: 1.0255, G_loss: 1.1554\n",
            "Epoch [3/10], Step [600/1094], D_loss: 1.0360, G_loss: 1.1141\n",
            "Epoch [3/10], Step [700/1094], D_loss: 1.1293, G_loss: 0.9900\n",
            "Epoch [3/10], Step [800/1094], D_loss: 1.1714, G_loss: 0.9488\n",
            "Epoch [3/10], Step [900/1094], D_loss: 1.0672, G_loss: 0.9652\n",
            "Epoch [3/10], Step [1000/1094], D_loss: 1.1186, G_loss: 1.0933\n",
            "Epoch [3/10] completed. Generated images saved.\n",
            "Epoch [4/10], Step [100/1094], D_loss: 1.1704, G_loss: 1.0235\n",
            "Epoch [4/10], Step [200/1094], D_loss: 1.0404, G_loss: 1.0629\n",
            "Epoch [4/10], Step [300/1094], D_loss: 1.0653, G_loss: 1.0191\n",
            "Epoch [4/10], Step [400/1094], D_loss: 1.1132, G_loss: 1.0768\n",
            "Epoch [4/10], Step [500/1094], D_loss: 1.1070, G_loss: 1.0485\n",
            "Epoch [4/10], Step [600/1094], D_loss: 1.2615, G_loss: 1.0623\n",
            "Epoch [4/10], Step [700/1094], D_loss: 1.0689, G_loss: 1.0279\n",
            "Epoch [4/10], Step [800/1094], D_loss: 1.0536, G_loss: 1.0955\n",
            "Epoch [4/10], Step [900/1094], D_loss: 1.1182, G_loss: 1.0847\n",
            "Epoch [4/10], Step [1000/1094], D_loss: 1.0250, G_loss: 1.1639\n",
            "Epoch [4/10] completed. Generated images saved.\n",
            "Epoch [5/10], Step [100/1094], D_loss: 1.0605, G_loss: 1.1302\n",
            "Epoch [5/10], Step [200/1094], D_loss: 0.9069, G_loss: 1.1952\n",
            "Epoch [5/10], Step [300/1094], D_loss: 1.0690, G_loss: 1.1095\n",
            "Epoch [5/10], Step [400/1094], D_loss: 1.0299, G_loss: 1.1692\n",
            "Epoch [5/10], Step [500/1094], D_loss: 0.9922, G_loss: 1.1838\n",
            "Epoch [5/10], Step [600/1094], D_loss: 1.1403, G_loss: 1.1203\n",
            "Epoch [5/10], Step [700/1094], D_loss: 1.0643, G_loss: 1.1984\n",
            "Epoch [5/10], Step [800/1094], D_loss: 1.1575, G_loss: 1.2382\n",
            "Epoch [5/10], Step [900/1094], D_loss: 0.8689, G_loss: 1.2705\n",
            "Epoch [5/10], Step [1000/1094], D_loss: 0.9702, G_loss: 1.2939\n",
            "Epoch [5/10] completed. Generated images saved.\n",
            "Epoch [6/10], Step [100/1094], D_loss: 1.0898, G_loss: 1.1042\n",
            "Epoch [6/10], Step [200/1094], D_loss: 1.0390, G_loss: 1.1704\n",
            "Epoch [6/10], Step [300/1094], D_loss: 0.9449, G_loss: 1.3781\n",
            "Epoch [6/10], Step [400/1094], D_loss: 0.8813, G_loss: 1.1546\n",
            "Epoch [6/10], Step [500/1094], D_loss: 1.0471, G_loss: 1.1057\n",
            "Epoch [6/10], Step [600/1094], D_loss: 1.1209, G_loss: 1.1558\n",
            "Epoch [6/10], Step [700/1094], D_loss: 0.9790, G_loss: 1.1409\n",
            "Epoch [6/10], Step [800/1094], D_loss: 1.0476, G_loss: 1.2010\n",
            "Epoch [6/10], Step [900/1094], D_loss: 0.9255, G_loss: 1.3328\n",
            "Epoch [6/10], Step [1000/1094], D_loss: 1.0483, G_loss: 1.2233\n",
            "Epoch [6/10] completed. Generated images saved.\n",
            "Epoch [7/10], Step [100/1094], D_loss: 1.0346, G_loss: 1.2531\n",
            "Epoch [7/10], Step [200/1094], D_loss: 0.9604, G_loss: 1.1468\n",
            "Epoch [7/10], Step [300/1094], D_loss: 1.2196, G_loss: 1.1262\n",
            "Epoch [7/10], Step [400/1094], D_loss: 0.9055, G_loss: 1.3148\n",
            "Epoch [7/10], Step [500/1094], D_loss: 0.9282, G_loss: 1.2616\n",
            "Epoch [7/10], Step [600/1094], D_loss: 1.0347, G_loss: 1.2241\n",
            "Epoch [7/10], Step [700/1094], D_loss: 0.9256, G_loss: 1.2106\n",
            "Epoch [7/10], Step [800/1094], D_loss: 1.0139, G_loss: 1.1514\n",
            "Epoch [7/10], Step [900/1094], D_loss: 1.1533, G_loss: 0.9554\n",
            "Epoch [7/10], Step [1000/1094], D_loss: 0.9943, G_loss: 1.2006\n",
            "Epoch [7/10] completed. Generated images saved.\n",
            "Epoch [8/10], Step [100/1094], D_loss: 1.0983, G_loss: 1.0843\n",
            "Epoch [8/10], Step [200/1094], D_loss: 0.9514, G_loss: 1.2749\n",
            "Epoch [8/10], Step [300/1094], D_loss: 1.1068, G_loss: 1.1237\n",
            "Epoch [8/10], Step [400/1094], D_loss: 1.1654, G_loss: 1.1533\n",
            "Epoch [8/10], Step [500/1094], D_loss: 0.9246, G_loss: 1.3299\n",
            "Epoch [8/10], Step [600/1094], D_loss: 0.8365, G_loss: 1.3753\n",
            "Epoch [8/10], Step [700/1094], D_loss: 0.9495, G_loss: 1.2848\n",
            "Epoch [8/10], Step [800/1094], D_loss: 0.9483, G_loss: 1.2847\n",
            "Epoch [8/10], Step [900/1094], D_loss: 1.1313, G_loss: 1.2757\n",
            "Epoch [8/10], Step [1000/1094], D_loss: 1.0143, G_loss: 1.2018\n",
            "Epoch [8/10] completed. Generated images saved.\n",
            "Epoch [9/10], Step [100/1094], D_loss: 1.1377, G_loss: 1.2871\n",
            "Epoch [9/10], Step [200/1094], D_loss: 1.0698, G_loss: 1.2442\n",
            "Epoch [9/10], Step [300/1094], D_loss: 1.0476, G_loss: 1.1350\n",
            "Epoch [9/10], Step [400/1094], D_loss: 0.9623, G_loss: 1.3277\n",
            "Epoch [9/10], Step [500/1094], D_loss: 0.8172, G_loss: 1.3249\n",
            "Epoch [9/10], Step [600/1094], D_loss: 1.0341, G_loss: 1.3347\n",
            "Epoch [9/10], Step [700/1094], D_loss: 0.9742, G_loss: 1.1768\n",
            "Epoch [9/10], Step [800/1094], D_loss: 0.8526, G_loss: 1.4879\n",
            "Epoch [9/10], Step [900/1094], D_loss: 0.9693, G_loss: 1.2681\n",
            "Epoch [9/10], Step [1000/1094], D_loss: 1.0053, G_loss: 1.3212\n",
            "Epoch [9/10] completed. Generated images saved.\n",
            "Epoch [10/10], Step [100/1094], D_loss: 0.9672, G_loss: 1.2207\n",
            "Epoch [10/10], Step [200/1094], D_loss: 0.9920, G_loss: 1.4256\n",
            "Epoch [10/10], Step [300/1094], D_loss: 0.8979, G_loss: 1.2897\n",
            "Epoch [10/10], Step [400/1094], D_loss: 1.0194, G_loss: 1.2942\n",
            "Epoch [10/10], Step [500/1094], D_loss: 1.0309, G_loss: 1.1608\n",
            "Epoch [10/10], Step [600/1094], D_loss: 1.0320, G_loss: 1.3141\n",
            "Epoch [10/10], Step [700/1094], D_loss: 1.0575, G_loss: 1.3150\n",
            "Epoch [10/10], Step [800/1094], D_loss: 0.9119, G_loss: 1.3129\n",
            "Epoch [10/10], Step [900/1094], D_loss: 1.0148, G_loss: 1.2388\n",
            "Epoch [10/10], Step [1000/1094], D_loss: 1.1482, G_loss: 1.2855\n",
            "Epoch [10/10] completed. Generated images saved.\n"
          ]
        }
      ]
    }
  ]
}